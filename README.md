# WhisperTest: A Voice-Control-based Library for iOS UI Automation
This repository contains the code for the paper titled ["WhisperTest: A Voice-Control-based Library for iOS UI Automation"](https://research.utwente.nl/files/508770849/moti-whispertest-2025.pdf) (to appear at [ACM CCS 2025](https://www.sigsac.org/ccs/CCS2025/)).


Built on top of [pymobiledevice3](https://github.com/doronz88/pymobiledevice3), this library leverages Apple's Voice Control accessibility feature to interact with iOS apps naturally and reliably.


## ğŸŒŸ Features

**ğŸ—£ï¸ Text-to-Speech + Voice Control**:
Automates app interaction using Appleâ€™s native Voice Control and spoken commands.

**ğŸ’» Cross-platform**:
Runs on macOS, Linux, and Windows.

**ğŸ Works on the latest iOS versions without requiring jailbreak**:
Fully compatible with iOS 17 and above.

**ğŸ“± Testing of third-party apps**:
Enables automation of any iOS app without developer access or modifications.

**ğŸ§© Modular and extensible architecture**:
Easily integrate new features or navigation strategies. 

**ğŸ” Comprehensive Data Collection**:
- ğŸ–¼ï¸ Screenshots â€” Captured at each interaction step
- ğŸ¥ Screen recordings â€” Full session video (MP4)
- ğŸŒ Network traffic â€” PCAP files for traffic and tracker analysis
- â™¿ Accessibility data â€” UI tree dumps and element metadata
- ğŸ”¤ OCR output â€” Extracted on-screen text and icons (via OmniParser)

## ğŸ“‹ Prerequisites

### iOS Device Setup

1. **Enable Voice Control**:
   - Go to Settings â†’ Accessibility â†’ Voice Control
   - Toggle on Voice Control

2. **Enable Developer Mode** (Perform developer operations (Requires enable of Developer-Mode)):
   - Settings â†’ Privacy & Security â†’ Developer Mode

3. **Trust Computer**:
   - Connect device via USB
   - Tap "Trust" when prompted on device

4. **Start Remote Service Tunnel** (iOS 17.4+):
   ```bash
   # Start the tunneld service (keeps running in background)
   sudo -E pymobiledevice3 remote tunneld
   
   # Or use the provided helper script
   ./whisper_test/scripts/start_tunnel.sh
   ```

> **Note**: The `tunneld` service must remain running for the framework to communicate with your device. Run it in a separate terminal window or as a background service.

### ğŸ”Œ External Services 

- **Omniparser OCR Service**: 
  WhisperTest integrates with a REST-based version of [OmniParser](https://github.com/zahra7394/OmniParser) â€” a FastAPI service that performs OCR and visual element detection on screenshots.  
  The service can run locally or remotely and returns structured detection results and a labeled image.

  **Quick start:**
  ```bash
  git clone https://github.com/zahra7394/OmniParser.git
  cd OmniParser
  pip install -r requirements.txt
  python app.py
  ```
  The API will start at http://localhost:5000/process.
  WhisperTest connects automatically if omniparser_api_url in config.json is set to this endpoint.

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/whispertest.git
cd whispertest
```

### 2. Install Python Dependencies

```bash
# Create a virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Install System Dependencies

#### Piper TTS (Recommended for better voice quality)

```bash
# Download from releases: https://github.com/rhasspy/piper/releases
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_amd64.tar.gz
tar -xvf piper_amd64.tar.gz
sudo mv piper /usr/local/bin/
```

**Download Voice Models**:
1. Visit [Piper Voices](https://github.com/rhasspy/piper/blob/master/VOICES.md)
2. Download desired models (e.g., `en_US-amy-medium`)
3. Place `.onnx` and `.onnx.json` files in the `piper/` directory

#### NLTK Data

```bash
python -m nltk.downloader punkt stopwords wordnet
```

### 4. Verify Installation

```bash
# Check if pymobiledevice3 can see your device
pymobiledevice3 usbmux list

# Should show your connected iOS device
```

### 5. Configure the Framework

Create a `config.json` file in the root directory to customize settings:

```json
{
  "media_path": "media_output",
  "tts_provider": "piper_en_US-amy-medium",
  "piper_root_dir": "piper",
  "consent_mode": "accept",
  "timeout_app_navigation": 200,
  "timeout_app_installation": 120,
  "omniparser_api_url": "api_url",
  "llm_api_url": "api_url"
}
```

**Configuration Options**:
- `media_path`: Directory to save screenshots, videos, and data
- `tts_provider`: TTS engine (`piper_en_US-amy-medium` or `gTTS`)
- `piper_root_dir`: Directory containing Piper voice models
- `consent_mode`: How to handle dialogs (`accept` or `reject`)
- `timeout_app_navigation`: Maximum time (seconds) for app navigation
- `omniparser_api_url`: URL for OmniParser OCR service (optional)
- `llm_api_url`: URL for LLM-based navigation service (optional)

## ğŸ“– Usage

### Quick Start Example

```python
from whisper_test.device import WhisperTestDevice

# Initialize device connection
device = WhisperTestDevice()

# Install an app from IPA file
device.install_app_via_ipa("path/to/app.ipa")

# Launch the app
app_bundle_id = "com.example.myapp"
device.launch_app(app_bundle_id)

# Take a screenshot and get screen content
screenshot, _ = device.take_screenshots(app_bundle_id)
a11y_data = device.get_screen_content_by_a11y()

# Issue voice commands
device.say("Tap Continue")
device.say("Scroll down")

# Clean up
device.uninstall_app(app_bundle_id)
device.close()
```


## ğŸ—ï¸ Architecture

### Core Components

- **`device.py`**: Main device interface and control
- **`navigation.py`**: App navigation 
- **`tts.py`**: Text-to-speech controller with multi-provider support
- **`data_collector.py`**: Automated data collection
- **`rule_based_app_navigation.py`**: Rule-based dialog and permission handling
- **`llm_based_app_navigation.py`**: LLM-powered intelligent navigation
- **`ocr_utils.py`**: OCR and visual element detection (OmniParser integration)
- **`a11y_utils.py`**: Accessibility and UI element extraction
- **`app_utils.py`**: App installation, launch, and management
- **`syslog_monitor.py`**: Real-time system log monitoring
- **`utils.py`**: General utility functions
- **`common.py`**: Configuration management and shared constants
- **`exceptions.py`**: Custom exception classes
- **`logger_config.py`**: Logging configuration

### Directory Structure

```
whispertest/
â”œâ”€â”€ examples/                    # Example scripts
â”‚   â”œâ”€â”€ data_collection/        # Data collection
â”‚   â”œâ”€â”€ get_installed_apps/     # List installed apps
â”‚   â”œâ”€â”€ launch_app/             # App launching
â”‚   â”œâ”€â”€ pcap/                   # Network capture
â”‚   â”œâ”€â”€ syslog/                 # Log monitoring
â”‚   â”œâ”€â”€ take_screenshot/        # Screenshot examples
â”‚   â””â”€â”€ web_automation/         # Web crawling
â”œâ”€â”€ whisper_test/               # Main library
â”‚   â”œâ”€â”€ test/                   # Test suite
â”‚   â””â”€â”€ scripts/                # Helper scripts
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


## ğŸ”§ Configuration

### TTS Providers

- **Piper** (Recommended): Offline, high-quality voices
- **gTTS**: Online, requires internet

Configure in `config.json`:

```json
{
  "tts_provider": "piper_en_US-amy-medium",
  "piper_root_dir": "piper"
}
```

### Consent Mode
Control how the library handles permission dialogs:

- `"accept"`: Accept all permissions (cookies, tracking, location, etc.)
- `"reject"`: Reject all permissions

### LLM Configuration
TBD

## ğŸ“Š Data Collection

WhisperTest automatically collects comprehensive data during app navigation:

- **Screenshots**: PNG images at each navigation step
- **Accessibility Data**: UI and screen element information
- **OCR Results**: Text and element positions from screens
- **Videos**: Screen recordings of entire app sessions
- **Network Traffic**: PCAP files of network activity

Output structure (one app generates multiple files at each navigation step):

```
media_output/
â”œâ”€â”€ com.example.app_20240101_120000.png
â”œâ”€â”€ com.example.app_ocr_20240101_120000.json
â”œâ”€â”€ com.example.app_a11y_20240101_120000.txt
â”œâ”€â”€ com.example.app_20240101_120030.png
â”œâ”€â”€ com.example.app_ocr_20240101_120030.json
â”œâ”€â”€ com.example.app_a11y_20240101_120030.txt
â”œâ”€â”€ ...
â”œâ”€â”€ com.example.app_20240101_120000.pcap    # One per session
â””â”€â”€ com.example.app_20240101_120000.mp4     # One per session

```

## ğŸ§ª Testing

Run the test suite:

```bash
pytest -sv whisper_test/test/
```

## ğŸ› Troubleshooting

**Problem**: `Cannot connect to device` or `No devices found`

**Solutions**:
1. Check physical connection:
   ```bash
   pymobiledevice3 usbmux list
   ```
2. Ensure tunneld is running (iOS 17+):
   ```bash
   sudo -E pymobiledevice3 remote tunneld
   ```
3. Verify device trust:
   - Disconnect and reconnect USB cable
   - Look for "Trust This Computer?" prompt on device
   - Enter device passcode


**Problem**: Voice commands not working or being ignored

**Solutions**:
1. Verify Voice Control is active
2. Test audio playback
3. Check TTS configuration
4. Adjust device volume
5. Try alternative TTS provider


## ğŸ“ Reference

```bibtex
@inproceedings{moti_whispertest_25,
 author = {Moti, Zahra and Janssen-Groesbeek, Tom and Monteiro, Steven and Continella, Andrea and Acar, Gunes},
 booktitle = {Proceedings of the ACM Conference on Computer and Communications Security (CCS)},
 month = {October},
 title = {WhisperTest: A Voice-Control-based Library for iOS UI Automation},
 year = {2025}
}

```

## ğŸ¤ Contributing

We welcome contributions! Whether it's bug fixes, new features, documentation improvements, your help is appreciated.

## ğŸ™ Acknowledgments

- **[pymobiledevice3](https://github.com/doronz88/pymobiledevice3)** - The foundation of this library. 

- **[Piper](https://github.com/rhasspy/piper)** - High-quality neural text-to-speech engine that enables natural voice commands with minimal latency.

- **[OmniParser](https://github.com/microsoft/OmniParser)** - Advanced OCR and UI element detection.

### Contact

For any questions, suggestions, or issues regarding this project or our paper, please contact:

| **Author**         | **Email**                     |
|--------------------|-------------------------------|
| [Zahra Moti](https://www.ru.nl/en/people/moti-jeshveghani-z)      | zahra.moti@ru.nl   |
| [Tom Janssen-Groesbeek](https://tomjanssengroesbeek.nl/)    | tom.janssen-groesbeek@ru.nl   |
| Steven Monteiro   | s.c.monteiro@student.utwente.nl   |
| [Gunes Acar](https://gunesacar.net/)     | g.acar@cs.ru.nl   |
| [Andrea Continella](https://conand.me/) | a.continella@utwente.nl |





